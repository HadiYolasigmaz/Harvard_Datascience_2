---
title: "Harvard_Task"
author: "Hadi Yolasigmaz"
date: "3/24/2019"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## INTRODUCTION

### Dataset

Dataset selected is one of the machine learning ready dataset downloaded from 'Kaggle.com'. It was in curated list of datasets

Subject of selected Dataset is 'Biomechanical Features of Orthopedic Patients'. Each patient in the data set (line); six biomechanics derived from the shape and orientation of the pelvis and the lumbar spine (each one is a column) are particularly represented: Pelvic incidence, Pelvic tilt numeric, Lumbar lordosis angle, Sacral slope, Pelvic radius, Degree spondylolisthesi.
  
It is a clean data. After data visulization, Multiclass Classification will be applied as machine learning analyses to this data.

Column_3C_weka.csv will be imported as dataset and it has 7 colummns data. First six columns are six biomechanics and last column is used to classify patients. In file, 100 patients are as normal, 60 patients are as Disk Hernia and 150 patients are Spondilolistez. Total 310 patients.  

### The goal of the project

The main goal of the project is to be known that we are ready to datascience but the specific goal of the project is to find a prediction way by checking Biomechanical Features of Orthopedic Patients and predict whether they are normal, 'Disk Hernia' or 'Spondilolistez' 

### Structure of data

```{r}
Data_3class <- read.csv(file="column_3C_weka.csv", header=TRUE, sep=",")
str(Data_3class)
```

### Summary of data

```{r}
summary(Data_3class)
```

### First 6 data lines

```{r}
head(Data_3class)
```

##ANALYSES

Firstly, rapid data analyses will be done  by visualization and lastly machine learning Multiclass Classification will be applied 

### Data Visulization

#### Distribution of Patients in 3 class items dataset

```{r eval=TRUE,echo =TRUE, results="hide", include=FALSE}
require(ggplot2)
require(gridExtra)
```

```{r}
ggplot(Data_3class,aes(x=class,fill=class))+geom_bar(stat = 'count')+labs(x = 'Distribution of patients') +
  geom_label(stat='count',aes(label=..count..), size=4) +theme_dark(base_size = 12)
```

```{r eval=TRUE,echo =TRUE, results="hide", include=FALSE}
require(GGally)
```

#### Pair graphs of data.

```{r}
ggpairs(data=Data_3class, columns=c(1:7))
```

It is seen that from diagonally looking there is an outlier problem for degree_spondylolisthesis. it should be corrected first by redefine it as mean of column, mean of degree_spondylolisthesis.

```{r}
outlier_3class <- which.max(Data_3class$degree_spondylolisthesis)
Data_3class$degree_spondylolisthesis[outlier_3class] <- mean(Data_3class$degree_spondylolisthesis)
```

Refreshing pair graph it is viewed that outlier problem is Solved.

```{r}
ggpairs(data=Data_3class, columns=c(1:7))
```

#### Correlation between six biometrics

It is better to see  this relations in number format not only graph format. 

```{r}
suppressMessages(library(corrplot))
corr_mat <- cor(Data_3class[,1:6])
corrplot(corr_mat, method = "number")
```

There aren't good correlations and relations between 6 biometrics that easily formalize.

### Machine learning techniques

As seen, it is not easily formalize, use mathematic relation we  will try to do well defined, well known machine learning techniques for this dataset to classify. 
Classification with More than Two Classes machine learning techniques will be applied. They are Decision trees and  random forest.

#### Prepare Train set and test set
Firstly, train dataset and test dataset will be prepared from analyses.

```{r eval=TRUE,echo =TRUE, results="hide", include=FALSE}
require(dplyr)
require(caret)
```

```{r}
y <- Data_3class$class
set.seed(1)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- Data_3class %>% slice(-test_index)
test_set <- Data_3class %>% slice(test_index)
```

#### Classification (decision) trees.

This is a technique that it can be controlled and has interpretability. we can follow all branches of decision tree. For this rpart.plot will be used visualization of decision tree.

```{r eval=TRUE,echo =TRUE, results="hide", include=FALSE}
require(rpart.plot)
```

```{r}
class.tree <- rpart(Data_3class$class~.,data = Data_3class,control = rpart.control(cp = 0.01))
rpart.plot(class.tree, 
           box.palette="GnBu",
           branch.lty=10, shadow.col="gray", nn=TRUE)
```

First branch of decision tree is degree_spondylolisthesis whether degree_spondylolisthesis is bigger than 16, If it is yes then patient class is spondylolisthesis.
Numbers in the box.
0.19 - 0.193548387 - Number of Hernia in the dataset - (60 / 310)
0.32 - 0.322580645 - Number of Normal in the dataset - (100 / 310)
0.48 - 0.483870968 - Number of spondylolisthesis in the dataset - (150 / 310)
 
Right side of the first branche is spondylolisthesis patients
48% of all data
Numbers in the box.
0.00 - 0 - Number of Hernia in the dataset - (0 / 150)
0.02 - 0.2 - Number of Normal in the dataset - (3 / 150)
0.98 - 0.98 - Number of spondylolisthesis in the dataset - (147 / 150)
and so on.

cp is selected  as 0.01 by controlling accuracy with respect to below approach.    

```{r}
train_rpart <- train(class ~ .,  method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 50)),
                     data = train_set)
ggplot(train_rpart)
```

And Accuracy is 

```{r}
confusionMatrix(predict(train_rpart, test_set),test_set$class)$overall["Accuracy"]
```

#####random forest

This is a technique that it can't be easily interpretability. Excepted give better accuracy in prediction.

```{r eval=TRUE,echo =TRUE, results="hide", include=FALSE}
require(randomForest)
```

```{r}
fit <- randomForest(class~., data = Data_3class, ntree=500, proximity=T) 
plot(fit)
fit.legend <- colnames(Data_3class)
legend("top", cex =0.3, legend=fit.legend, lty=c(1,2,3,4,5,6,7), col=c(1,2,3,4,5,6,7), horiz=T)
```

Around 100 is the value can be used. It is better to find lowest err.rate. It is 100.

```{r}
which.min(fit$err.rate[,1])
```

```{r}
suppressMessages(library(randomForest))
rf.train_set <-randomForest(class ~., data=train_set)
rf.train_set
```

Accuracy is better as expected.

```{r}
train_rf <- randomForest(class ~ ., data=train_set)
confusionMatrix(predict(train_rf, test_set), test_set$class)$overall["Accuracy"]
```

## Results

When Decision Tree is used. Accuracy was 

```{r}
confusionMatrix(predict(train_rpart, test_set),test_set$class)$overall["Accuracy"]
```

When Random Forest is used. Accuracy was 

```{r}
confusionMatrix(predict(train_rf, test_set), test_set$class)$overall["Accuracy"]
```

## CONCLUSION

Of course, to have a predictionwith accuracies above 0.8, it can be seen as good. 
But if we talk about patients, 2 things should be done. 
One side is Biometric data collection side. It should be  continue to understand whether there is any misadded variable that affect patients conditions. Amount or way of data collection should be changed.
Another side  is Datascience part, to apply methods to data, may be sometimes new grouping from owned data, may be sometimes only concentrate one part of problem but at the end come to a point that increase accuracy of owned data. Prediction should be say more. This is time consuming operations that sometimes, I believe this, work together is better to predict more close.

